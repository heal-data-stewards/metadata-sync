{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This jupyter notebook will develp the SOP for updating the Monday.com studies board consistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = Path(\"/Users/hinashah/Documents/HEAL/MondayFolderUpdate_202406/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_values(df:pd.DataFrame, col_name:str='appl_id'):\n",
    "    if col_name in df.columns:\n",
    "        return df[ ~pd.isna(df[col_name])][col_name].drop_duplicates()\n",
    "    return None\n",
    "\n",
    "def get_na_count(df:pd.DataFrame, col_name:str='appl_id'):\n",
    "    if col_name in df.columns:\n",
    "        return len(df[pd.isna(df[col_name])])\n",
    "    return -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1615\n",
      "Index(['appl_id', 'xstudy_id', 'study_most_recent_appl', 'study_hdp_id',\n",
      "       'study_hdp_id_appl'],\n",
      "      dtype='object')\n",
      "appl_id                   object\n",
      "xstudy_id                 object\n",
      "study_most_recent_appl    object\n",
      "study_hdp_id              object\n",
      "study_hdp_id_appl         object\n",
      "dtype: object\n",
      "Number of distinct values in --appl_id--: 1606\n",
      "---- NA count: 0\n",
      "Number of funky looking appl_ids: 0\n",
      "Number of distinct values in --xstudy_id--: 1313\n",
      "---- NA count: 0\n",
      "Number of distinct values in --study_most_recent_appl--: 1304\n",
      "---- NA count: 0\n",
      "Number of funky looking appl_ids: 0\n",
      "Number of distinct values in --study_hdp_id--: 1273\n",
      "---- NA count: 53\n",
      " Number of funky looking HDPIDs: 0\n",
      "Number of distinct values in --study_hdp_id_appl--: 1264\n",
      "---- NA count: 53\n",
      "Number of funky looking appl_ids: 0\n"
     ]
    }
   ],
   "source": [
    "gt_file = pd.read_csv(input_dir/\"study_lookup_table.csv\", dtype=str)\n",
    "gt_file.replace(\"0\", np.NaN, inplace=True)\n",
    "\n",
    "print(len(gt_file))\n",
    "print(gt_file.columns)\n",
    "print(gt_file.dtypes)\n",
    "### QC the file:\n",
    "for k in gt_file.columns:\n",
    "    print(f\"Number of distinct values in --{k}--: {len(get_unique_values(gt_file, k))}\")\n",
    "    print(f\"---- NA count: {get_na_count(gt_file, k)}\")\n",
    "    ## Look for patterns?\n",
    "    if 'appl' in k:\n",
    "        d = gt_file[[ (not pd.isna(l)) and (not l.isdigit()) for l in gt_file[k] ]]\n",
    "        print(f\"Number of funky looking appl_ids: {len(d)}\")\n",
    "    elif k == 'study_hdp_id':\n",
    "        d = gt_file[ [ (not pd.isna(l)) and (re.match(r'HDP[\\d]+', l) is None) for l in gt_file[k]]]\n",
    "        print(f\" Number of funky looking HDPIDs: {len(d)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count          907\n",
      "unique         907\n",
      "top       HDP00001\n",
      "freq             1\n",
      "Name: Name, dtype: object\n",
      "Index(['Name', 'Most Recent Appl_ID', 'HDP appl_ID', 'Project #', 'Archived',\n",
      "       'HEAL-Related', 'Research Focus', 'Research Program', 'TEMP_Res_Prog',\n",
      "       'Title', 'Contact PI', 'Contact Email', 'Administering IC', 'NIH PO',\n",
      "       'Institution(s)', 'PI(s)', 'Location', 'Activity Code', 'Award Type',\n",
      "       'Award Year', 'Total Funded', 'Summary', 'Project Start', 'Project End',\n",
      "       'Reporter Link', 'Network', 'Data Mgmt', 'DAI Import Status',\n",
      "       'Data Engagement', '\"Get the Data\" Engagement Board', 'Repo per PI',\n",
      "       'Repo per Platform', 'Platform Reg Time', 'CEDAR Form %',\n",
      "       'Repo Mapping', 'repo_22_2', 'repo_22_3', 'Creation Log'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "## Import Monday Board \n",
    "## TODO: change to read in all groups\n",
    "monday_board = pd.read_excel(input_dir/\"HEAL_Studies_HEAL_Studies_1718802245.xlsx\", skiprows=4, dtype={\"Most Recent Appl_ID\":str}, skipfooter=1)\n",
    "print(monday_board[\"Name\"].describe())\n",
    "print(monday_board.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number records from Monday already in lookup table: 907\n",
      "Number records from Monday that are not in lookup table: 0\n",
      "Number records from l0ookup that are not in Monday: 406\n"
     ]
    }
   ],
   "source": [
    "### Steps for updating Monday board:\n",
    "\n",
    "## From Study lookup table, get unique set of most_recent_appl, study_hdp_id, and study_hdp_id_appl\n",
    "lookup_fields = gt_file[['study_hdp_id', 'study_most_recent_appl', 'study_hdp_id_appl']].copy(deep=True).drop_duplicates()\n",
    "## Create a column \"Name\" or \"Key\" that will either have study_hdp_id OR most_recent_appl when study_hdp_id is empty\n",
    "lookup_fields['key'] = [m if pd.isna(h) else h for (h, m) in lookup_fields[['study_hdp_id', 'study_most_recent_appl']].values ]\n",
    "\n",
    "### A few checks:\n",
    "## How many of the \"keys\" from Monday board are in lookup fields?\n",
    "print(f\"Number records from Monday already in lookup table: {len(monday_board[monday_board.Name.isin(lookup_fields.key)])}\")\n",
    "## How many of the keys from MOnday board are not there in looup fields\n",
    "mondayboard_missingin_looup = monday_board[~monday_board.Name.isin(lookup_fields.key)]\n",
    "print(f\"Number records from Monday that are not in lookup table: {len(mondayboard_missingin_looup)}\")\n",
    "## How many of the keys from lookup fields are not there in Monday??\n",
    "lookup_missingin_mondayboard = lookup_fields[~lookup_fields.key.isin(monday_board.Name)]\n",
    "print(f\"Number records from l0ookup that are not in Monday: {len(lookup_missingin_mondayboard)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Most Recent Appl_ID</th>\n",
       "      <th>HDP appl_ID</th>\n",
       "      <th>Project #</th>\n",
       "      <th>Archived</th>\n",
       "      <th>HEAL-Related</th>\n",
       "      <th>Research Focus</th>\n",
       "      <th>Research Program</th>\n",
       "      <th>TEMP_Res_Prog</th>\n",
       "      <th>Title</th>\n",
       "      <th>...</th>\n",
       "      <th>Data Engagement</th>\n",
       "      <th>\"Get the Data\" Engagement Board</th>\n",
       "      <th>Repo per PI</th>\n",
       "      <th>Repo per Platform</th>\n",
       "      <th>Platform Reg Time</th>\n",
       "      <th>CEDAR Form %</th>\n",
       "      <th>Repo Mapping</th>\n",
       "      <th>repo_22_2</th>\n",
       "      <th>repo_22_3</th>\n",
       "      <th>Creation Log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Name, Most Recent Appl_ID, HDP appl_ID, Project #, Archived, HEAL-Related, Research Focus, Research Program, TEMP_Res_Prog, Title, Contact PI, Contact Email, Administering IC, NIH PO, Institution(s), PI(s), Location, Activity Code, Award Type, Award Year, Total Funded, Summary, Project Start, Project End, Reporter Link, Network, Data Mgmt, DAI Import Status, Data Engagement, \"Get the Data\" Engagement Board, Repo per PI, Repo per Platform, Platform Reg Time, CEDAR Form %, Repo Mapping, repo_22_2, repo_22_3, Creation Log]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 38 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mondayboard_missingin_looup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awards table has: 1615 entries, with 1615 appl_ids\n",
      "Reporter table has: 1617 entries, with 1617 appl_ids\n",
      "Platform generated table has: 1283 entries, with 1271 appl_ids\n",
      "Platform table has 1271 unique HDP IDs\n",
      "Repo mapping table has: 1323 entrie, with 1323 appl_ids\n",
      "Repo mapping table has: 1059 entrie, with 1059 appl_ids\n",
      "Research Network table has: 1613 entrie, with 1613 appl_ids\n"
     ]
    }
   ],
   "source": [
    "# Get rest of the tables\n",
    "convert_dict = {'appl_id':str}\n",
    "\n",
    "awards_df = pd.read_csv(input_dir/\"awards.csv\", low_memory=False, dtype=convert_dict)\n",
    "awards_df = awards_df.dropna(how='all')\n",
    "print(f\"Awards table has: {len(awards_df)} entries, with {len(get_unique_values(awards_df))} appl_ids\")\n",
    "reporter_df = pd.read_csv(input_dir/\"reporter.csv\", low_memory=False, dtype=convert_dict)\n",
    "reporter_df = reporter_df.dropna(how='all')\n",
    "print(f\"Reporter table has: {len(reporter_df)} entries, with {len(get_unique_values(reporter_df))} appl_ids\")\n",
    "progress_tracker_df = pd.read_csv(input_dir/\"progress_tracker.csv\", low_memory=False, dtype=convert_dict)\n",
    "print(f\"Platform generated table has: {len(progress_tracker_df)} entries, with {len(get_unique_values(progress_tracker_df))} appl_ids\")\n",
    "print(f\"Platform table has {len(get_unique_values(progress_tracker_df))} unique HDP IDs\")\n",
    "repo_maping_df = pd.read_csv(input_dir/\"repo_mapping.csv\", low_memory=False, dtype=convert_dict)\n",
    "print(f\"Repo mapping table has: {len(repo_maping_df)} entrie, with {len(get_unique_values(repo_maping_df))} appl_ids\")\n",
    "pi_emails_df = pd.read_csv(input_dir/\"pi_emails.csv\", low_memory=False, dtype=convert_dict)\n",
    "print(f\"Repo mapping table has: {len(pi_emails_df)} entrie, with {len(get_unique_values(pi_emails_df))} appl_ids\")\n",
    "resnet_df = pd.read_csv(input_dir/\"research_networks.csv\", low_memory=False, dtype=convert_dict)\n",
    "print(f\"Research Network table has: {len(resnet_df)} entrie, with {len(get_unique_values(resnet_df))} appl_ids\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1606\n",
      "     study_most_recent_appl                   pi_email_latest\n",
      "1                   9755001                 kwatkins@rand.org\n",
      "2                   9850412                   damico@rand.org\n",
      "4                  10478911                 LYNN.DEBAR@KP.ORG\n",
      "8                  10468778          cheville.andrea@mayo.edu\n",
      "11                 10054792                   xcao11@jhmi.edu\n",
      "...                     ...                               ...\n",
      "1593               10167785               bahmedani@yahoo.com\n",
      "1594               10331849                  tbrocki1@JHU.EDU\n",
      "1596               10197811                  kzivin@UMICH.EDU\n",
      "1597               10197809            Gregory.E.Simon@kp.org\n",
      "1599                9823898  d-mencihella@md.northwestern.edu\n",
      "\n",
      "[962 rows x 2 columns]\n",
      "count    961.000000\n",
      "mean       1.001041\n",
      "std        0.032258\n",
      "min        1.000000\n",
      "25%        1.000000\n",
      "50%        1.000000\n",
      "75%        1.000000\n",
      "max        2.000000\n",
      "dtype: float64\n",
      "1606\n",
      "     study_most_recent_appl                  pi_email\n",
      "0                   9860408                          \n",
      "1                   9755001         kwatkins@rand.org\n",
      "2                   9850412           damico@rand.org\n",
      "4                  10478911         LYNN.DEBAR@KP.ORG\n",
      "8                  10468778  cheville.andrea@mayo.edu\n",
      "...                     ...                       ...\n",
      "1601               10022491                          \n",
      "1602               10493291                          \n",
      "1603                9555046                          \n",
      "1604                9775470                          \n",
      "1605               10589995                          \n",
      "\n",
      "[1304 rows x 2 columns]\n",
      "     study_most_recent_appl                  pi_email  \\\n",
      "0                   9860408                             \n",
      "1                   9755001         kwatkins@rand.org   \n",
      "2                   9850412           damico@rand.org   \n",
      "3                  10478911         LYNN.DEBAR@KP.ORG   \n",
      "4                  10468778  cheville.andrea@mayo.edu   \n",
      "...                     ...                       ...   \n",
      "1299               10022491                             \n",
      "1300               10493291                             \n",
      "1301                9555046                             \n",
      "1302                9775470                             \n",
      "1303               10589995                             \n",
      "\n",
      "                 Contact Email          pi_email_updated  \n",
      "0                                                         \n",
      "1            kwatkins@rand.org         kwatkins@rand.org  \n",
      "2              damico@rand.org           damico@rand.org  \n",
      "3            LYNN.DEBAR@KP.ORG         LYNN.DEBAR@KP.ORG  \n",
      "4     cheville.andrea@mayo.edu  cheville.andrea@mayo.edu  \n",
      "...                        ...                       ...  \n",
      "1299                                                      \n",
      "1300                                                      \n",
      "1301                                                      \n",
      "1302                                                      \n",
      "1303                                                      \n",
      "\n",
      "[1304 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tb/c_qhpk1j2jj3knzfnw72yj080000gq/T/ipykernel_35611/1495752057.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  appl_ids_emails['pi_email'].fillna('', inplace=True)\n",
      "/var/folders/tb/c_qhpk1j2jj3knzfnw72yj080000gq/T/ipykernel_35611/1495752057.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  pi_emails_df_updated_monday['Contact Email'].replace('-', '', inplace=True)\n",
      "/var/folders/tb/c_qhpk1j2jj3knzfnw72yj080000gq/T/ipykernel_35611/1495752057.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  pi_emails_df_updated_monday['Contact Email'].fillna('', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "## Manipulate emails to carry forward emails from a previous appl_id to the most recent one according to the lookup table and email table\n",
    "appl_ids = gt_file[['appl_id', 'study_most_recent_appl']].drop_duplicates()\n",
    "print(len(appl_ids))\n",
    "appl_ids_emails = pd.merge(appl_ids, pi_emails_df, how='left', on='appl_id')\n",
    "\n",
    "most_recent_emails = appl_ids_emails[ ~pd.isna(appl_ids_emails.pi_email)][['study_most_recent_appl', 'pi_email']].drop_duplicates()\n",
    "most_recent_emails.rename(columns={'pi_email':'pi_email_latest'}, inplace=True)\n",
    "print(most_recent_emails)\n",
    "email_counts = most_recent_emails.groupby('study_most_recent_appl').size()\n",
    "appl_ids_counts = appl_ids_emails.groupby('study_most_recent_appl').size()\n",
    "\n",
    "print(email_counts.describe())\n",
    "appl_ids_emails['email_count'] = [email_counts[k] if k in email_counts else 0 for k in appl_ids_emails['study_most_recent_appl']]\n",
    "appl_ids_emails['applid_count'] = [appl_ids_counts[k] if k in appl_ids_counts else 0 for k in appl_ids_emails['study_most_recent_appl']]\n",
    "appl_ids_emails['pi_email'].fillna('', inplace=True)\n",
    "appl_ids_emails['keep'] = [1 if (c==0 or (c==1 and len(e)>0) or (c>1 and a==m)) else 0 for (c,a,m,e) in appl_ids_emails[['email_count', 'appl_id', 'study_most_recent_appl', 'pi_email' ]].values]\n",
    "print(len(appl_ids_emails))\n",
    "\n",
    "pi_emails_df_updated = appl_ids_emails[appl_ids_emails['keep']==1][['study_most_recent_appl', 'pi_email']].drop_duplicates()\n",
    "pi_emails_df_updated['pi_email'] = [k.strip() for k in pi_emails_df_updated['pi_email']]\n",
    "print(pi_emails_df_updated)\n",
    "\n",
    "## Get Monday board emails, and fill in any that are different from mysql..\n",
    "pi_emails_df_updated_monday = pd.merge(pi_emails_df_updated, monday_board[['Most Recent Appl_ID', 'Contact Email']].drop_duplicates(), how='left', left_on='study_most_recent_appl', right_on='Most Recent Appl_ID').drop(columns='Most Recent Appl_ID')\n",
    "pi_emails_df_updated_monday['Contact Email'].replace('-', '', inplace=True)\n",
    "pi_emails_df_updated_monday['Contact Email'].fillna('', inplace=True)\n",
    "pi_emails_df_updated_monday['pi_email_updated'] = [me if (len(e)==0 and len(me) > 1) else e for (e,me) in pi_emails_df_updated_monday[['pi_email', 'Contact Email']].values]\n",
    "print(pi_emails_df_updated_monday)\n",
    "pi_emails_df_updated_monday.to_csv(input_dir/\"email_updates.csv\", index=False)\n",
    "appl_ids_emails.to_csv(input_dir/\"email_counts.csv\", index=False)\n",
    "\n",
    "pi_emails_df_updated = pi_emails_df_updated_monday[['study_most_recent_appl', 'pi_email_updated']].rename(columns={'pi_email_updated':'pi_email'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Collect fields from report/awards tables that are required by Monday Board\n",
    "rename_dict = {'proj_num':'Project #', \n",
    "               'proj_title':'Title',\n",
    "                'rfa':'Research Focus',\n",
    "                'res_prg':'Research Program',\n",
    "                'ctc_pi_nm':'Contact PI',\n",
    "                'pi_email':'Contact Email',\n",
    "                'adm_ic':'Administering IC',\n",
    "                'prg_ofc':'NIH PO',\n",
    "                'org_nm': 'Institution(s)',\n",
    "                'pi':'PI(s)',\n",
    "                'org_cy':'City',\n",
    "                'org_st':'State',\n",
    "                'act_code':'Activity Code',\n",
    "                'awd_ty':'Award Type',\n",
    "                'fisc_yr':'Award Year',\n",
    "                'tot_fund':'Total Funded',\n",
    "                'proj_abs':'Summary',\n",
    "                'fund_mech': 'SBIR/STTR',\n",
    "                'dai_res':'DAI Import Status',\n",
    "                'proj_strt_date':'Project Start',\n",
    "                'proj_end_date':'Project End',\n",
    "                'proj_url':'Reporter Link',\n",
    "                'res_net':'Network',\n",
    "                'repo_22_1':'Repo Mapping',\n",
    "                'repo_22_2':'repo_22_2',\n",
    "                'repo_22_3':'repo_22_3',\n",
    "                'time_of_registration':'Platform Reg Time',\n",
    "                'overall_percent_complete':'CEDAR Form %',\n",
    "                'repository_name' : 'Repo per Platform',\n",
    "                'archived':'Archived',\n",
    "                'heal_funded':'HEAL-Related'\n",
    "                }\n",
    "\n",
    "def create_mysql_subset(in_df:pd.DataFrame, extra_fields = ['appl_id']):\n",
    "    subset = in_df[[k for k in rename_dict.keys() if k in in_df.columns] + extra_fields].copy(deep=True)\n",
    "    subset.rename(columns={k:v for k,v in rename_dict.items() if k in in_df.columns}, inplace=True)\n",
    "    return subset\n",
    "    \n",
    "mysql_fields_reporter = create_mysql_subset(awards_df)\n",
    "mysql_fields_awards = create_mysql_subset(reporter_df)\n",
    "myql_fields_repomapping = create_mysql_subset(repo_maping_df)\n",
    "mysql_fields_platform = create_mysql_subset(progress_tracker_df, extra_fields=['hdp_id'])\n",
    "mysql_fields_piemails = create_mysql_subset(pi_emails_df_updated, extra_fields=['study_most_recent_appl'])\n",
    "mysql_fields_resnet = create_mysql_subset(resnet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1313\n",
      "1313\n",
      "1313\n",
      "1313\n",
      "1313\n",
      "1313\n",
      "1313\n",
      "1313\n"
     ]
    }
   ],
   "source": [
    "print(len(lookup_fields))\n",
    "data_merge_1 = pd.merge(lookup_fields, mysql_fields_reporter, how='left', left_on='study_most_recent_appl', right_on='appl_id').drop(columns='appl_id')\n",
    "print(len(data_merge_1))\n",
    "data_merge_2 = pd.merge(data_merge_1, mysql_fields_awards, how='left', left_on='study_most_recent_appl', right_on='appl_id').drop(columns='appl_id')\n",
    "print(len(data_merge_2))\n",
    "data_merge_1 = pd.merge(data_merge_2, myql_fields_repomapping, how='left', left_on='study_most_recent_appl', right_on='appl_id').drop(columns='appl_id')\n",
    "print(len(data_merge_1))\n",
    "data_merge_2 = pd.merge(data_merge_1, mysql_fields_platform, how='left', left_on='study_hdp_id', right_on='hdp_id')\n",
    "print(len(data_merge_2))\n",
    "data_merge_1 = pd.merge(data_merge_2, mysql_fields_resnet, how='left', left_on='study_most_recent_appl', right_on='appl_id').drop(columns='appl_id')\n",
    "print(len(data_merge_1))\n",
    "combined_data = pd.merge(data_merge_1, mysql_fields_piemails, how='left', on='study_most_recent_appl')\n",
    "print(len(combined_data))\n",
    "print(len(combined_data.drop_duplicates()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of empty values for each of the fields gathered:\n",
      "Project # : 0\n",
      "Title : 0\n",
      "Research Focus : 15\n",
      "Research Program : 79\n",
      "Contact PI : 0\n",
      "Contact Email : 0\n",
      "Administering IC : 0\n",
      "NIH PO : 58\n",
      "Institution(s) : 0\n",
      "PI(s) : 0\n",
      "City : 5\n",
      "State : 7\n",
      "Activity Code : 0\n",
      "Award Type : 0\n",
      "Award Year : 0\n",
      "Total Funded : 0\n",
      "Summary : 8\n",
      "SBIR/STTR : 0\n",
      "DAI Import Status : 754\n",
      "Project Start : 0\n",
      "Project End : 0\n",
      "Reporter Link : 0\n",
      "Network : 791\n",
      "Repo Mapping : 744\n",
      "repo_22_2 : 1204\n",
      "repo_22_3 : 1305\n",
      "Platform Reg Time : 964\n",
      "CEDAR Form % : 40\n",
      "Repo per Platform : 1161\n",
      "Archived : 40\n",
      "HEAL-Related : 26\n"
     ]
    }
   ],
   "source": [
    "## Find out which columns have NA values, and investigate for incompletemess?\n",
    "print(\"Number of empty values for each of the fields gathered:\")\n",
    "for k in rename_dict.values():\n",
    "    print(f\"{k} : {get_na_count(combined_data, k)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Most Recent Appl_ID', 'HDP appl_ID', 'key', 'Research Focus',\n",
       "       'Research Program', 'DAI Import Status', 'HEAL-Related', 'Project #',\n",
       "       'Title', 'Contact PI', 'Administering IC', 'NIH PO', 'Institution(s)',\n",
       "       'PI(s)', 'City', 'State', 'Activity Code', 'Award Type', 'Award Year',\n",
       "       'Total Funded', 'Summary', 'SBIR/STTR', 'Project Start', 'Project End',\n",
       "       'Reporter Link', 'Repo Mapping', 'repo_22_2', 'repo_22_3',\n",
       "       'Platform Reg Time', 'CEDAR Form %', 'Repo per Platform', 'Archived',\n",
       "       'Network', 'Contact Email', 'Location', 'study_type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create a column named \"Location\"dd\n",
    "from datetime import datetime\n",
    "combined_data['City'] = combined_data[['City']].fillna('')\n",
    "combined_data['State'] = combined_data[['State']].fillna('')\n",
    "combined_data['Location'] = [c+\",\"+s for (c,s) in combined_data[['City', \"State\"]].values]\n",
    "\n",
    "combined_data['Project Start'] = pd.to_datetime(combined_data['Project Start'], format='%Y-%m-%d', errors='coerce').dt.date\n",
    "combined_data['Project End'] = pd.to_datetime(combined_data['Project End'], format='%Y-%m-%d', errors='coerce').dt.date\n",
    "combined_data['Platform Reg Time'] = pd.to_datetime(combined_data['Platform Reg Time'], utc=True).dt.date\n",
    "\n",
    "\n",
    "combined_data['Archived'] = [a if a=='archived' else '' for a in combined_data['Archived']]\n",
    "combined_data['HEAL-Related'] = ['Y' if pd.isna(a) else '' for a in combined_data['HEAL-Related']]\n",
    "combined_data['SBIR/STTR'] = [t if t=='SBIR/STTR' else '' for t in combined_data['SBIR/STTR']]\n",
    "## TODO: add condition here to identify CTN studies\n",
    "combined_data['study_type'] = ['APPLIDONLY' if pd.isna(k) else 'HDP' for k in combined_data['key']]\n",
    "\n",
    "## Rename a few of the other columns:\n",
    "combined_data.rename(columns={'study_most_recent_appl':'Most Recent Appl_ID', 'study_hdp_id_appl':'HDP appl_ID'}, inplace=True)\n",
    "combined_data.drop(columns=['study_hdp_id', 'hdp_id'], inplace=True)\n",
    "\n",
    "combined_data.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Find what's in Monday.com board, but not in mysql extract\n",
    "# Mark these entries for deletion, and these would have to be deleted manually on Monday.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making sure uniqueness of key values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    1313.0\n",
       "mean        1.0\n",
       "std         0.0\n",
       "min         1.0\n",
       "25%         1.0\n",
       "50%         1.0\n",
       "75%         1.0\n",
       "max         1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data.index.name = 'index'\n",
    "combined_data.to_excel(input_dir/\"MondayBoard_Update.xlsx\")\n",
    "\n",
    "print(\"Making sure uniqueness of key values\")\n",
    "key_counts = combined_data.groupby('key').size()\n",
    "key_counts.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_data_subset = combined_data[ combined_data['Most Recent Appl_ID'].isin(monday_board['Most Recent Appl_ID']) | combined_data['key'].isin(monday_board['Name'])].drop_duplicates()\n",
    "# combined_data_subset = combined_data_subset[ ~pd.isna(combined_data_subset['Most Recent Appl_ID']) ]\n",
    "# print(len(combined_data_subset))\n",
    "# combined_data_subset.to_excel(input_dir/\"MondayBoard_Update_Step2.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(monday_board))\n",
    "# missing_monday = combined_data_subset[~combined_data_subset.key.isin(monday_board.Name)]\n",
    "# print(len(missing_monday))\n",
    "# print(missing_monday.key)\n",
    "\n",
    "# ## Did all the most_recent_appl_ids make it into the subset\n",
    "# print(f\"Number of most recent applids on Monday: {len(monday_board['Most Recent Appl_ID'].drop_duplicates())}, in subset: {len(combined_data_subset['Most Recent Appl_ID'].drop_duplicates())}\")\n",
    "# print(monday_board[ ~monday_board['Most Recent Appl_ID'].isin(combined_data_subset['Most Recent Appl_ID']) ]['Most Recent Appl_ID'])\n",
    "\n",
    "# ## What is in Monday that is not in this combined dataset?\n",
    "# missing_frommonday_inextract = monday_board[~monday_board.Name.isin(combined_data_subset.key)]\n",
    "# print(len(missing_frommonday_inextract))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*******************\n",
    "*******************\n",
    "DEBUG CODE BELOW\n",
    "*******************\n",
    "*******************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Most Recent Appl_ID</th>\n",
       "      <th>HDP appl_ID</th>\n",
       "      <th>key</th>\n",
       "      <th>Research Focus</th>\n",
       "      <th>Research Program</th>\n",
       "      <th>DAI Import Status</th>\n",
       "      <th>HEAL-Related</th>\n",
       "      <th>Project #</th>\n",
       "      <th>Title</th>\n",
       "      <th>Contact PI</th>\n",
       "      <th>...</th>\n",
       "      <th>repo_22_2</th>\n",
       "      <th>repo_22_3</th>\n",
       "      <th>Platform Reg Time</th>\n",
       "      <th>CEDAR Form %</th>\n",
       "      <th>Repo per Platform</th>\n",
       "      <th>Archived</th>\n",
       "      <th>Network</th>\n",
       "      <th>Contact Email</th>\n",
       "      <th>Location</th>\n",
       "      <th>study_type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>10593312</td>\n",
       "      <td>10601172</td>\n",
       "      <td>HDP00889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>3R24DA055306-02S1</td>\n",
       "      <td>Wake Forest IMPOWR Dissemination Education and...</td>\n",
       "      <td>ADAMS, MEREDITH C. B.</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-07-26</td>\n",
       "      <td>5.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>meradams@wakehealth.edu</td>\n",
       "      <td>WINSTON-SALEM,NC</td>\n",
       "      <td>HDP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Most Recent Appl_ID HDP appl_ID       key Research Focus  \\\n",
       "index                                                            \n",
       "660              10593312    10601172  HDP00889            NaN   \n",
       "\n",
       "      Research Program DAI Import Status HEAL-Related          Project #  \\\n",
       "index                                                                      \n",
       "660                NaN               NaN            Y  3R24DA055306-02S1   \n",
       "\n",
       "                                                   Title  \\\n",
       "index                                                      \n",
       "660    Wake Forest IMPOWR Dissemination Education and...   \n",
       "\n",
       "                  Contact PI  ... repo_22_2 repo_22_3 Platform Reg Time  \\\n",
       "index                         ...                                         \n",
       "660    ADAMS, MEREDITH C. B.  ...       NaN       NaN        2022-07-26   \n",
       "\n",
       "      CEDAR Form % Repo per Platform Archived Network  \\\n",
       "index                                                   \n",
       "660            5.8               NaN              NaN   \n",
       "\n",
       "                 Contact Email          Location  study_type  \n",
       "index                                                         \n",
       "660    meradams@wakehealth.edu  WINSTON-SALEM,NC         HDP  \n",
       "\n",
       "[1 rows x 36 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data[combined_data['key']=='HDP00889'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_cols = ['Most Recent Appl_ID', 'HDP appl_ID', 'Contact Email', 'Network']\n",
    "\n",
    "comparison_df = pd.merge(monday_board[['Name'] +comparison_cols ], combined_data[['key'] + comparison_cols], left_on = 'Name', right_on='key').drop_duplicates()\n",
    "comparison_df.to_csv(input_dir/\"comparison.csv\", index=False)\n",
    "comparison_df.to_excel(input_dir/\"comparison.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Name', 'Most Recent Appl_ID_x', 'HDP appl_ID_x', 'Contact Email_x',\n",
       "       'Network_x', 'key', 'Most Recent Appl_ID_y', 'HDP appl_ID_y',\n",
       "       'Contact Email_y', 'Network_y'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study_most_recent_appl</th>\n",
       "      <th>pi_email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>9901704</td>\n",
       "      <td>jhambm@upmc.edu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    study_most_recent_appl         pi_email\n",
       "881                9901704  jhambm@upmc.edu"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_emails_df_updated[ pi_emails_df_updated.study_most_recent_appl=='9901704']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>appl_id</th>\n",
       "      <th>res_net</th>\n",
       "      <th>res_net_override_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1486</th>\n",
       "      <td>9908734</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      appl_id res_net  res_net_override_flag\n",
       "1486  9908734     NaN                      0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_df[resnet_df.appl_id=='9908734']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.to_excel(input_dir/\"test.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/hinashah/Documents/HEAL/MondayFolderUpdate_202406/STEP1_MondayUpdate.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m expected_study_groups \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dir\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSTEP1_MondayUpdate.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m study_groups_to_keep \u001b[38;5;241m=\u001b[39m expected_study_groups[expected_study_groups\u001b[38;5;241m.\u001b[39mkeep_count\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mmost_recent_study_group\u001b[38;5;241m.\u001b[39mdrop_duplicates()\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m      4\u001b[0m study_groups \u001b[38;5;241m=\u001b[39m monday_board[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTEMP: most_recent_study_group\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/hinashah/Documents/HEAL/MondayFolderUpdate_202406/STEP1_MondayUpdate.csv'"
     ]
    }
   ],
   "source": [
    "expected_study_groups = pd.read_csv(input_dir/\"STEP1_MondayUpdate.csv\", low_memory=False)\n",
    "study_groups_to_keep = expected_study_groups[expected_study_groups.keep_count==1].most_recent_study_group.drop_duplicates().values\n",
    "\n",
    "study_groups = monday_board['TEMP: most_recent_study_group']\n",
    "missing_studygroups = [k for k in study_groups_to_keep if k not in study_groups.values]\n",
    "print(len(missing_studygroups))\n",
    "missing_studygroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['study_hdp_id', 'study_most_recent_appl', 'study_hdp_id_appl', 'key',\n",
       "       'Research Focus', 'TEMP_Res_Prog', 'Data Mgmt', 'DAI Import Status',\n",
       "       'Network', 'Is Heal Funded or Relevant', 'Project #', 'Title',\n",
       "       'Conctact PI', 'Administering IC', 'NIH PO', 'Institution(s)', 'PI(s)',\n",
       "       'City', 'State', 'Activity Code', 'Award Type', 'Total Funded',\n",
       "       'Summary', 'Project Start', 'Project End', 'Reporter Link',\n",
       "       'Repo Mapping', 'repo_22_2', 'repo_22_3', 'Platform Reg Time',\n",
       "       'CEDAR Form %', 'Repo per Platform', 'Archived on Platform?', 'hdp_id',\n",
       "       'Contact Email', 'Location'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
